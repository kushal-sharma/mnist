# mnist
Deep learning concepts tried out on mnist

# GAN
Simple GAN architecture in gan.py. Generator network has two layers with leaky Relu activation with hidden layer having 128 units. 
Optimizer used is Adam. Discriminator and generator trained alternatively for 0-8. '9' proves to be a harder one to learn so discriminator to generator training steps is 10:1.

## Results for gan - 
